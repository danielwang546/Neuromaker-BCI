{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import antropy as ant\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, roc_auc_score, \n",
    "                            precision_score, recall_score, f1_score, classification_report, \n",
    "                            roc_curve, plot_roc_curve, auc, precision_recall_curve, \n",
    "                            plot_precision_recall_curve, average_precision_score)\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin, end = 1, 61 # (begin is inclusive, end is exclusive)\n",
    "num_people = 14\n",
    "count_samples = {\n",
    "    \"active\": 8,\n",
    "    \"meditate\": 8,\n",
    "    \"neutral\": 8\n",
    "}\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self):\n",
    "        self.data = {\n",
    "            'RawEEG': [],\n",
    "            'Alpha': [],\n",
    "            'Low Beta': [],\n",
    "            'High Beta': [],\n",
    "            'Gamma': [],\n",
    "            'Theta': [],\n",
    "            'Delta': [],\n",
    "            'Meditation': [],\n",
    "            'Attention': []\n",
    "        }\n",
    "\n",
    "    def recordDataPoint(self, RawEEG, Attention, Meditation, Alpha, Delta, Theta, LowBeta, HighBeta, Gamma):\n",
    "        self.data['RawEEG'].append(float(RawEEG))\n",
    "        self.data['Attention'].append(float(Attention))\n",
    "        self.data['Meditation'].append(float(Meditation))\n",
    "        self.data['Alpha'].append(float(Alpha))\n",
    "        self.data['Delta'].append(float(Delta))\n",
    "        self.data['Theta'].append(float(Theta))\n",
    "        self.data['Low Beta'].append(float(LowBeta))\n",
    "        self.data['High Beta'].append(float(HighBeta))\n",
    "        self.data['Gamma'].append(float(Gamma))\n",
    "\n",
    "    '''\n",
    "    Record a line of data from the CSV output, which takes form RawEEG, Alpha, Delta, Gamma, Low Beta, High Beta, Theta, Attention, Meditation\n",
    "\n",
    "    '''\n",
    "    def recordDataLine(self, line):\n",
    "        self.recordDataPoint(line[0], line[7], line[8], line[1], line[2], line[6], line[4], line[5], line[3])\n",
    "    \n",
    "    def getEEG(self):\n",
    "        return self.data['RawEEG']\n",
    "    \n",
    "    def getAttention(self):\n",
    "        return self.data[\"Attention\"]\n",
    "    \n",
    "    def getMeditation(self):\n",
    "        return self.data[\"Meditation\"]\n",
    "    \n",
    "    def getAlpha(self):\n",
    "        return self.data[\"Alpha\"]\n",
    "    \n",
    "    def getDelta(self):\n",
    "        return self.data[\"Delta\"]\n",
    "    \n",
    "    def getTheta(self):\n",
    "        return self.data[\"Theta\"]\n",
    "    \n",
    "    def getLowBeta(self):\n",
    "        return self.data[\"Low Beta\"]\n",
    "    \n",
    "    def getHighBeta(self):\n",
    "        return self.data[\"High Beta\"]\n",
    "    \n",
    "    def getGamma(self):\n",
    "        return self.data[\"Gamma\"]\n",
    "\n",
    "    def get(self, key):\n",
    "        return self.data[key]\n",
    "\n",
    "    '''\n",
    "    Filter out all outliers, as defined by being outside 3*std from the mean, and replace with mean of the samples around them\n",
    "    '''\n",
    "    def filter_outliers(self):\n",
    "        sampleBad = False\n",
    "        for key in ['RawEEG', 'Alpha', 'Theta', 'Low Beta', 'High Beta', \"Gamma\", 'Delta']:\n",
    "            data = self.data[key]\n",
    "            \n",
    "            filtered = []\n",
    "\n",
    "            iqr = np.subtract(*np.percentile(data, [75, 25]))\n",
    "            med = np.median(data)\n",
    "\n",
    "            for x in data:\n",
    "                \n",
    "                if (med - 1.5*iqr > x) or (med + 1.5*iqr < x) or abs(x - np.mean(data)) > 2 * np.std(data):\n",
    "                    filtered.append(med)\n",
    "                    # filtered.append(np.median(data[max(0, i-5):i] + data[i+1:min(len(data), i+5)]))\n",
    "                else:\n",
    "                    filtered.append(x)\n",
    "                    \n",
    "            self.data[key] = filtered\n",
    "        return sampleBad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {personNum : {state: [sampleNums]}}\n",
    "# 0 = key for throwing away all samples of that state\n",
    "\n",
    "badSamples = {\n",
    "    1: {\"active\": [5], \"neutral\": [2], \"meditate\": []},\n",
    "    2: {\"active\": [0], \"neutral\": [0], \"meditate\": [0]},\n",
    "    3: {\"active\": [1, 4], \"neutral\": [1], \"meditate\": [5, 6, 7, 8]},\n",
    "    4: {\"active\": [2], \"neutral\": [1, 7], \"meditate\": [1, 8]},\n",
    "    5: {\"active\": [], \"neutral\": [], \"meditate\": []}, \n",
    "    6: {\"active\": [], \"neutral\": [2, 6], \"meditate\": []},\n",
    "    7: {\"active\": [5], \"neutral\": [4, 6, 7], \"meditate\": [1, 3, 4, 8]}, \n",
    "    8: {\"active\": [5], \"neutral\": [1], \"meditate\": [5, 8]}, \n",
    "    9: {\"active\": [], \"neutral\": [], \"meditate\": []}, \n",
    "    10: {\"active\": [6, 8], \"neutral\": [4, 5, 6], \"meditate\": []},\n",
    "    11: {\"active\": [4], \"neutral\": [4, 8], \"meditate\": [1, 2, 3, 5, 7]},\n",
    "    12: {\"active\": [2, 3, 8], \"neutral\": [0], \"meditate\": [6]}, \n",
    "    13: {\"active\": [], \"neutral\": [8], \"meditate\": []},\n",
    "    14: {\"active\": [4, 5, 8], \"neutral\": [0], \"meditate\": [1, 2, 8]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "dataLabels = []\n",
    "\n",
    "def transcribeFileToSample(personN: int, sampleN: int, state: str, X, y, outlierFiltering = True):\n",
    "    sample_data = Sample()\n",
    "\n",
    "    with open(\"data/all_data/\" + state + \"_\" + str(personN) + \"_\" + str(sampleN) + \".csv\") as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            sample_data.recordDataLine(row)\n",
    "\n",
    "        if (outlierFiltering):   \n",
    "            if (0 not in badSamples[personN][state] and sampleN not in badSamples[personN][state]):\n",
    "\n",
    "                for key in sample_data.data:\n",
    "                    sample_data.data[key] = sample_data.data[key][begin:end]\n",
    "\n",
    "                sample_data.filter_outliers()\n",
    "                X.append(sample_data)\n",
    "                y.append(state)\n",
    "        else:\n",
    "            X.append(sample_data)\n",
    "            y.append(state)\n",
    "\n",
    "for person in range(num_people):\n",
    "    for state in count_samples:\n",
    "        for i in range(count_samples[state]):\n",
    "            transcribeFileToSample(person + 1, i + 1, state, data, dataLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExtracted = []\n",
    "\n",
    "def safety_check(x):\n",
    "    if math.isnan(x): return 0\n",
    "    if math.isinf(x): return 99999999999\n",
    "    return x\n",
    "\n",
    "for point in data:\n",
    "    extractedPoint = []\n",
    "\n",
    "    extractedPoint.append(np.mean(point.getAlpha()))\n",
    "    extractedPoint.append(np.mean(point.getLowBeta()))\n",
    "    extractedPoint.append(np.mean(point.getHighBeta())) \n",
    "    extractedPoint.append(np.mean(point.getGamma())) \n",
    "    extractedPoint.append(np.mean(point.getTheta()))\n",
    "    extractedPoint.append(np.std(point.getHighBeta())) \n",
    "    extractedPoint.append(np.std(point.getGamma()))\n",
    "    extractedPoint.append(np.std(point.getDelta()))\n",
    "    extractedPoint.append(safety_check(ant.sample_entropy(point.getDelta())))\n",
    "    extractedPoint.append(np.std(point.getLowBeta())) \n",
    "    extractedPoint.append(np.std(point.getTheta()))\n",
    "    \n",
    "    # extractedPoint.append(safety_check(ant.spectral_entropy(point.getEEG(), sf=1)))\n",
    "    # extractedPoint.append(np.mean(point.getDelta()))\n",
    "    # extractedPoint.append(np.std(point.getAlpha())) \n",
    "\n",
    "    dataExtracted.append(extractedPoint)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataExtracted, dataLabels, test_size=0.2, random_state=13, stratify=dataLabels)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Training for Individual Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', LogisticRegression(max_iter=1000))]), \n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', KNeighborsClassifier())]),\n",
    "    Pipeline([('classifier', GaussianNB())]),\n",
    "    Pipeline([('classifier', BernoulliNB())]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', DecisionTreeClassifier())]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', RandomForestClassifier())]),\n",
    "    # Pipeline([('ss', StandardScaler()), ('classifier', XGBClassifier())]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', GradientBoostingClassifier())]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', SVC(probability=True, max_iter=1000))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, \n",
    "                                                max_features=None, min_samples_leaf=1, \n",
    "                                                min_samples_split=2, random_state=0)\n",
    "    ))]),\n",
    "]\n",
    "\n",
    "model_grids = [\n",
    "               [{'classifier__C':[1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1, 5, 1e1, 5e1, 1e2, 5e2, 1e3],\n",
    "                 'classifier__random_state':[0]}],                                 #logistic regression\n",
    "               \n",
    "               [{'classifier__n_neighbors':[5,7,9,11, 13, 15, 17, 19], \n",
    "                 'classifier__metric': ['euclidean', 'manhattan', 'minkowski']}],  #KNN\n",
    "               \n",
    "               [{'classifier__var_smoothing': [1e-10, 1e-09, 1e-8, 1e-7]}],        #GaussianNB\n",
    "\n",
    "               [{'classifier__alpha': [1e-2, 1e-1, 1, 1e1, 1e2]}],                 #BernoulliNB\n",
    "\n",
    "               [{'classifier__criterion':['gini','entropy'],\n",
    "                 'classifier__random_state':[0], \n",
    "                 'classifier__max_depth' : [3, 5, 8, 10, 15, None], \n",
    "                 'classifier__min_samples_split' : [1.0,2,5,10,15,30],\n",
    "                 'classifier__min_samples_leaf': [1,2,5,10], \n",
    "                 'classifier__max_features': ['log2', 'sqrt', None]}],\n",
    "#Decision Tree\n",
    "               \n",
    "               [{'classifier__criterion':['gini','entropy'],\n",
    "                 'classifier__n_estimators': [1000],\n",
    "                 'classifier__random_state':[0], \n",
    "                 'classifier__max_depth' : [3, 5, 8, 10, 15, None], \n",
    "                 'classifier__min_samples_split' : [1.0,2,5,10,15,30],\n",
    "                 'classifier__min_samples_leaf': [1,2,5,10], \n",
    "                 'classifier__max_features': ['log2', 'sqrt', None]}],             #Random Forest\n",
    "\n",
    "              #  [{'classifier__n_estimators':[1000],\n",
    "              #    'classifier__criterion':['gini','entropy'],\n",
    "              #    'classifier__random_state':[0],\n",
    "              #    'classifier__max_depth': [3, 5, 8, 10, 15, 30],\n",
    "              #    'classifier__min_child_weight': [2,4,6,8,10],\n",
    "              #    'classifier__gamma': [0, 0.1, 0.2, 0.3],\n",
    "              #    'classifier__reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              #    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "              #    'classifier__eta': [0.1, 0.2, 0.3, 0.4, 0.5]}],                   #XGBoost\n",
    "\n",
    "                [{'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2], \n",
    "                 'classifier__n_estimators': [1000],\n",
    "                 'classifier__random_state':[0],\n",
    "                 'classifier__max_depth' : [5, 8, 15, None], \n",
    "                 'classifier__min_samples_split' : [1.0,2,5,10],\n",
    "                 'classifier__min_samples_leaf': [1,2,5,10], \n",
    "                 'classifier__max_features': ['log2', 'sqrt', 'auto', 'None']}],   #Gradient Bossting Decision Tree\n",
    "                        \n",
    "\n",
    "               [{'classifier__C':[1e-1, 1, 1e1] ,\n",
    "                 'classifier__random_state':[0],\n",
    "                 'classifier__kernel': ['rbf', 'poly']\n",
    "                }],                                                                #SVM\n",
    "               \n",
    "               [{'classifier__n_estimators' : [800, 1000, 1200], \n",
    "                 'classifier__learning_rate' : [1e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1, 1e1],\n",
    "                 'classifier__random_state':[0]}]                                  #AdaBoost\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out so it doesn't run every time I run all and waste a lot of time\n",
    "\n",
    "# for i,j in zip(models, model_grids):\n",
    "#     grid = RandomizedSearchCV(estimator=i, param_distributions=j, n_iter = 100, scoring='f1_weighted', cv = skf, n_jobs=-1)\n",
    "#     grid.fit(X_train, y_train)\n",
    "#     best_f1 = grid.best_score_\n",
    "#     best_param = grid.best_params_\n",
    "#     print('{}:\\nBest F1 : {:.4f}'.format(i.steps[-1],best_f1))\n",
    "#     print('Best Parameters : ',best_param)\n",
    "#     print('')\n",
    "#     print('----------------')\n",
    "#     print('')\n",
    "\n",
    "# Logistic Regression: F1 0.567, C=500.0\n",
    "# KNN: F1 0.686, n_neighbors=5, metric=\"manhattan\"\n",
    "# GaussianNB: F1 0.540, var_smoothing = 1e-10\n",
    "# BernoulliNB: F1 0.194, alpha=0.01\n",
    "# Decision Tree: F1 0.694, min_samples_split=2, min_samples_leaf=2, max_features=log2, max_depth=None, criterion=entropy\n",
    "# Random Forest: F1 idk, n_estimators: 1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=None, criterion=entropy\n",
    "# Gradient Boosting: F1 0.767, n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=log2, max_depth=8, learning_rate=0.2\n",
    "# SVC: F1 0.67, kernel=rbf, C=10.0\n",
    "# Ada Boost: F1: 0.603, n_estimators=800, learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all models with their tuned hyperparameters\n",
    "models_tuned = [\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', LogisticRegression(max_iter=1000, C=500.0))]), \n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', KNeighborsClassifier(n_neighbors=5, metric=\"manhattan\"))]),\n",
    "    Pipeline([('classifier', GaussianNB(var_smoothing=1e-10))]),\n",
    "    Pipeline([('classifier', BernoulliNB(alpha=0.01))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=2, \n",
    "                                                                            max_features=\"log2\", criterion=\"entropy\"))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', RandomForestClassifier(n_estimators=1000, min_samples_split=2, \n",
    "                                                                            min_samples_leaf=1, max_features=\"sqrt\", criterion=\"entropy\"))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', GradientBoostingClassifier(n_estimators=1000, min_samples_split=2,\n",
    "                                                                                min_samples_leaf=2, max_features=\"log2\",\n",
    "                                                                                max_depth=8, learning_rate=0.2))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', SVC(probability=True, max_iter=1000, kernel=\"rbf\", C=10.0))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, \n",
    "                                                max_features=None, min_samples_leaf=1, \n",
    "                                                min_samples_split=2, random_state=0), n_estimators=800, learning_rate=0.001))])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics/Scores for Individual Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression :\n",
      "Accuracy Score: 0.5918\n",
      "K-Fold Validation Mean Accuracy: 58.4615 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.61      0.61      0.61        18\n",
      "    meditate       0.75      0.71      0.73        17\n",
      "     neutral       0.40      0.43      0.41        14\n",
      "\n",
      "    accuracy                           0.59        49\n",
      "   macro avg       0.59      0.58      0.58        49\n",
      "weighted avg       0.60      0.59      0.60        49\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "KNeighborsClassifier :\n",
      "Accuracy Score: 0.6735\n",
      "K-Fold Validation Mean Accuracy: 69.2308 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.67      0.67      0.67        18\n",
      "    meditate       0.80      0.71      0.75        17\n",
      "     neutral       0.56      0.64      0.60        14\n",
      "\n",
      "    accuracy                           0.67        49\n",
      "   macro avg       0.68      0.67      0.67        49\n",
      "weighted avg       0.68      0.67      0.68        49\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "GaussianNB :\n",
      "Accuracy Score: 0.6122\n",
      "K-Fold Validation Mean Accuracy: 56.9231 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.60      0.50      0.55        18\n",
      "    meditate       0.56      0.88      0.68        17\n",
      "     neutral       0.86      0.43      0.57        14\n",
      "\n",
      "    accuracy                           0.61        49\n",
      "   macro avg       0.67      0.60      0.60        49\n",
      "weighted avg       0.66      0.61      0.60        49\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "BernoulliNB :\n",
      "Accuracy Score: 0.3673\n",
      "K-Fold Validation Mean Accuracy: 36.4103 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.37      1.00      0.54        18\n",
      "    meditate       0.00      0.00      0.00        17\n",
      "     neutral       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.37        49\n",
      "   macro avg       0.12      0.33      0.18        49\n",
      "weighted avg       0.13      0.37      0.20        49\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "DecisionTreeClassifier :\n",
      "Accuracy Score: 0.6735\n",
      "K-Fold Validation Mean Accuracy: 66.1538 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.58      0.83      0.68        18\n",
      "    meditate       0.90      0.53      0.67        17\n",
      "     neutral       0.69      0.64      0.67        14\n",
      "\n",
      "    accuracy                           0.67        49\n",
      "   macro avg       0.72      0.67      0.67        49\n",
      "weighted avg       0.72      0.67      0.67        49\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "RandomForestClassifier :\n",
      "Accuracy Score: 0.8571\n",
      "K-Fold Validation Mean Accuracy: 71.7949 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.76      0.89      0.82        18\n",
      "    meditate       1.00      0.88      0.94        17\n",
      "     neutral       0.85      0.79      0.81        14\n",
      "\n",
      "    accuracy                           0.86        49\n",
      "   macro avg       0.87      0.85      0.86        49\n",
      "weighted avg       0.87      0.86      0.86        49\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "GradientBoostingClassifier :\n",
      "Accuracy Score: 0.8163\n",
      "K-Fold Validation Mean Accuracy: 75.3846 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.82      0.78      0.80        18\n",
      "    meditate       1.00      0.88      0.94        17\n",
      "     neutral       0.65      0.79      0.71        14\n",
      "\n",
      "    accuracy                           0.82        49\n",
      "   macro avg       0.82      0.82      0.82        49\n",
      "weighted avg       0.83      0.82      0.82        49\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "SVC :\n",
      "Accuracy Score: 0.6327\n",
      "K-Fold Validation Mean Accuracy: 68.2051 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.61      0.61      0.61        18\n",
      "    meditate       0.87      0.76      0.81        17\n",
      "     neutral       0.44      0.50      0.47        14\n",
      "\n",
      "    accuracy                           0.63        49\n",
      "   macro avg       0.64      0.63      0.63        49\n",
      "weighted avg       0.65      0.63      0.64        49\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "AdaBoostClassifier :\n",
      "Accuracy Score: 0.7755\n",
      "K-Fold Validation Mean Accuracy: 61.0256 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.74      0.94      0.83        18\n",
      "    meditate       0.92      0.71      0.80        17\n",
      "     neutral       0.69      0.64      0.67        14\n",
      "\n",
      "    accuracy                           0.78        49\n",
      "   macro avg       0.78      0.76      0.77        49\n",
      "weighted avg       0.79      0.78      0.77        49\n",
      "\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuned_model_stats = []\n",
    "\n",
    "for m in range(len(models_tuned)):\n",
    "    model_stats = []\n",
    "    model = models_tuned[m]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = skf)   #K-Fold Validation\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average= 'weighted')\n",
    "    recall = recall_score(y_test, y_pred, average= 'weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average= 'weighted')\n",
    "\n",
    "    predicted_probab = model.predict_proba(X_test)\n",
    "    predicted_probab = predicted_probab[:, 1]\n",
    "   \n",
    "    print(type(models_tuned[m][-1]).__name__ , ':')\n",
    "    \n",
    "    print('Accuracy Score: {:.4f}'.format(test_acc))\n",
    "    print(\"K-Fold Validation Mean Accuracy: {:.4f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Classification report: \")\n",
    "    print(cr)\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    print('')\n",
    "    model_stats.append(type(models_tuned[m][-1]).__name__)\n",
    "    model_stats.append(accuracies.mean())\n",
    "    model_stats.append(test_acc)\n",
    "    model_stats.append(precision)\n",
    "    model_stats.append(recall)\n",
    "    model_stats.append(f1)\n",
    "    tuned_model_stats.append(model_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Cross-val Acc</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.868580</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.859472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.834334</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.821898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.610256</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.789571</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.772656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.683163</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.676531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.721978</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.672233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.682051</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.650170</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.639711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.658050</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.600186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.598980</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.595036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.364103</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.134944</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.197380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Cross-val Acc  Test Accuracy  Precision  \\\n",
       "5      RandomForestClassifier       0.717949       0.857143   0.868580   \n",
       "6  GradientBoostingClassifier       0.753846       0.816327   0.834334   \n",
       "8          AdaBoostClassifier       0.610256       0.775510   0.789571   \n",
       "1        KNeighborsClassifier       0.692308       0.673469   0.683163   \n",
       "4      DecisionTreeClassifier       0.661538       0.673469   0.721978   \n",
       "7                         SVC       0.682051       0.632653   0.650170   \n",
       "2                  GaussianNB       0.569231       0.612245   0.658050   \n",
       "0          LogisticRegression       0.584615       0.591837   0.598980   \n",
       "3                 BernoulliNB       0.364103       0.367347   0.134944   \n",
       "\n",
       "     Recall        F1  \n",
       "5  0.857143  0.859472  \n",
       "6  0.816327  0.821898  \n",
       "8  0.775510  0.772656  \n",
       "1  0.673469  0.676531  \n",
       "4  0.673469  0.672233  \n",
       "7  0.632653  0.639711  \n",
       "2  0.612245  0.600186  \n",
       "0  0.591837  0.595036  \n",
       "3  0.367347  0.197380  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tuned = pd.DataFrame(tuned_model_stats, columns= ['Model','Cross-val Acc','Test Accuracy','Precision','Recall', 'F1'])\n",
    "df_tuned.sort_values(by= ['F1'], inplace= True, ascending= False)\n",
    "df_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add SGDClassifier and/or MLPClassifier??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_classifiers = {\n",
    "    # ('log reg', Pipeline([('ss', StandardScaler()), ('classifier', LogisticRegression(max_iter=1000, C=500.0))])), \n",
    "    # ('gaussianNB', Pipeline([('classifier', GaussianNB(var_smoothing=1e-10))])),\n",
    "    # ('bernoulliNB', Pipeline([('classifier', BernoulliNB(alpha=0.01))])),\n",
    "    ('decistion tree', Pipeline([('ss', StandardScaler()), ('classifier', DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=2, \n",
    "                                                                            max_features=\"log2\", criterion=\"entropy\"))])), \n",
    "    ('random forest', Pipeline([('ss', StandardScaler()), ('classifier', RandomForestClassifier(n_estimators=1000, min_samples_split=2, \n",
    "                                                                            min_samples_leaf=1, max_features=\"sqrt\", criterion=\"entropy\"))])), \n",
    "    ('gradient boosting', Pipeline([('ss', StandardScaler()), ('classifier', GradientBoostingClassifier(n_estimators=1000, min_samples_split=2,\n",
    "                                                                                min_samples_leaf=2, max_features=\"log2\",\n",
    "                                                                                max_depth=8, learning_rate=0.2))])), \n",
    "    ('svc', Pipeline([('ss', StandardScaler()), ('classifier', SVC(probability=True, max_iter=1000, kernel=\"rbf\", C=10.0))])), \n",
    "    ('ada boost', Pipeline([('ss', StandardScaler()), ('classifier', AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, \n",
    "                                                max_features=None, min_samples_leaf=1, \n",
    "                                                min_samples_split=2, random_state=0), n_estimators=800, learning_rate=0.001))]))\n",
    "}\n",
    "\n",
    "final_est = final_estimator=RandomForestClassifier(criterion='entropy', n_estimators= 1000, random_state= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7755\n",
      "K-Fold Validation Mean Accuracy: 71.7949 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.70      0.78      0.74        18\n",
      "    meditate       1.00      0.82      0.90        17\n",
      "     neutral       0.67      0.71      0.69        14\n",
      "\n",
      "    accuracy                           0.78        49\n",
      "   macro avg       0.79      0.77      0.78        49\n",
      "weighted avg       0.79      0.78      0.78        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stackingClf = StackingClassifier(estimators=indiv_classifiers, final_estimator=final_est)\n",
    "\n",
    "stackingClf.fit(X_train, y_train)\n",
    "y_pred = stackingClf.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "accuracies = cross_val_score(estimator = stackingClf, X = X_train, y = y_train, cv = skf)   #K-Fold Validation\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average= 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average= 'weighted')\n",
    "f1 = f1_score(y_test, y_pred, average= 'weighted')\n",
    "\n",
    "model_stats = []\n",
    "model_stats.append(type(stackingClf).__name__)\n",
    "model_stats.append(accuracies.mean())\n",
    "model_stats.append(test_acc)\n",
    "model_stats.append(precision)\n",
    "model_stats.append(recall)\n",
    "model_stats.append(f1)\n",
    "tuned_model_stats.append(model_stats)\n",
    "\n",
    "print('Accuracy Score: {:.4f}'.format(test_acc))\n",
    "print(\"K-Fold Validation Mean Accuracy: {:.4f} %\".format(accuracies.mean()*100))\n",
    "print(\"Classification report: \")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8367\n",
      "K-Fold Validation Mean Accuracy: 68.7179 %\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      active       0.80      0.89      0.84        18\n",
      "    meditate       1.00      0.82      0.90        17\n",
      "     neutral       0.73      0.79      0.76        14\n",
      "\n",
      "    accuracy                           0.84        49\n",
      "   macro avg       0.84      0.83      0.83        49\n",
      "weighted avg       0.85      0.84      0.84        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votingClf = VotingClassifier(estimators=indiv_classifiers, voting='soft', weights = [1, 1.5, 1.4, 1, 1])\n",
    "# tree, forest, gradient, svc, ada\n",
    "\n",
    "votingClf.fit(X_train, y_train)\n",
    "y_pred = votingClf.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "accuracies = cross_val_score(estimator = votingClf, X = X_train, y = y_train, cv = skf, n_jobs=-1)   #K-Fold Validation\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average= 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average= 'weighted')\n",
    "f1 = f1_score(y_test, y_pred, average= 'weighted')\n",
    "\n",
    "model_stats = []\n",
    "model_stats.append(type(votingClf).__name__)\n",
    "model_stats.append(accuracies.mean())\n",
    "model_stats.append(test_acc)\n",
    "model_stats.append(precision)\n",
    "model_stats.append(recall)\n",
    "model_stats.append(f1)\n",
    "tuned_model_stats.append(model_stats)\n",
    "\n",
    "print('Accuracy Score: {:.4f}'.format(test_acc))\n",
    "print(\"K-Fold Validation Mean Accuracy: {:.4f} %\".format(accuracies.mean()*100))\n",
    "print(\"Classification report: \")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Cross-val Acc</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.868580</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.859472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VotingClassifier</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.850340</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.839458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.834334</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.821898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.794558</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.781085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.610256</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.789571</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.772656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.683163</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.676531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.721978</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.672233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.682051</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.650170</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.639711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.658050</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.600186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.598980</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.595036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.364103</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.134944</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.197380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  Cross-val Acc  Test Accuracy  Precision  \\\n",
       "5       RandomForestClassifier       0.717949       0.857143   0.868580   \n",
       "10            VotingClassifier       0.687179       0.836735   0.850340   \n",
       "6   GradientBoostingClassifier       0.753846       0.816327   0.834334   \n",
       "9           StackingClassifier       0.717949       0.775510   0.794558   \n",
       "8           AdaBoostClassifier       0.610256       0.775510   0.789571   \n",
       "1         KNeighborsClassifier       0.692308       0.673469   0.683163   \n",
       "4       DecisionTreeClassifier       0.661538       0.673469   0.721978   \n",
       "7                          SVC       0.682051       0.632653   0.650170   \n",
       "2                   GaussianNB       0.569231       0.612245   0.658050   \n",
       "0           LogisticRegression       0.584615       0.591837   0.598980   \n",
       "3                  BernoulliNB       0.364103       0.367347   0.134944   \n",
       "\n",
       "      Recall        F1  \n",
       "5   0.857143  0.859472  \n",
       "10  0.836735  0.839458  \n",
       "6   0.816327  0.821898  \n",
       "9   0.775510  0.781085  \n",
       "8   0.775510  0.772656  \n",
       "1   0.673469  0.676531  \n",
       "4   0.673469  0.672233  \n",
       "7   0.632653  0.639711  \n",
       "2   0.612245  0.600186  \n",
       "0   0.591837  0.595036  \n",
       "3   0.367347  0.197380  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tuned = pd.DataFrame(tuned_model_stats, columns= ['Model','Cross-val Acc','Test Accuracy','Precision','Recall', 'F1'])\n",
    "df_tuned.sort_values(by= ['F1'], inplace= True, ascending= False)\n",
    "df_tuned"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cb6d1e253a7ab3e8a07d0a3da00ac1f2b77bfa839ee5db5b3eaf4eb76ff3570"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
