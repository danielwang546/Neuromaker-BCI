{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import antropy as ant\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, roc_auc_score, \n",
    "                            precision_score, recall_score, f1_score, classification_report, \n",
    "                            roc_curve, plot_roc_curve, auc, precision_recall_curve, \n",
    "                            plot_precision_recall_curve, average_precision_score)\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin, end = 1, 61 # (begin is inclusive, end is exclusive)\n",
    "num_people = 14\n",
    "count_samples = {\n",
    "    \"active\": 8,\n",
    "    \"meditate\": 8,\n",
    "    \"neutral\": 8\n",
    "}\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self):\n",
    "        self.data = {\n",
    "            'RawEEG': [],\n",
    "            'Alpha': [],\n",
    "            'Low Beta': [],\n",
    "            'High Beta': [],\n",
    "            'Gamma': [],\n",
    "            'Theta': [],\n",
    "            'Delta': [],\n",
    "            'Meditation': [],\n",
    "            'Attention': []\n",
    "        }\n",
    "\n",
    "    def recordDataPoint(self, RawEEG, Attention, Meditation, Alpha, Delta, Theta, LowBeta, HighBeta, Gamma):\n",
    "        self.data['RawEEG'].append(float(RawEEG))\n",
    "        self.data['Attention'].append(float(Attention))\n",
    "        self.data['Meditation'].append(float(Meditation))\n",
    "        self.data['Alpha'].append(float(Alpha))\n",
    "        self.data['Delta'].append(float(Delta))\n",
    "        self.data['Theta'].append(float(Theta))\n",
    "        self.data['Low Beta'].append(float(LowBeta))\n",
    "        self.data['High Beta'].append(float(HighBeta))\n",
    "        self.data['Gamma'].append(float(Gamma))\n",
    "\n",
    "    '''\n",
    "    Record a line of data from the CSV output, which takes form RawEEG, Alpha, Delta, Gamma, Low Beta, High Beta, Theta, Attention, Meditation\n",
    "\n",
    "    '''\n",
    "    def recordDataLine(self, line):\n",
    "        self.recordDataPoint(line[0], line[7], line[8], line[1], line[2], line[6], line[4], line[5], line[3])\n",
    "    \n",
    "    def getEEG(self):\n",
    "        return self.data['RawEEG']\n",
    "    \n",
    "    def getAttention(self):\n",
    "        return self.data[\"Attention\"]\n",
    "    \n",
    "    def getMeditation(self):\n",
    "        return self.data[\"Meditation\"]\n",
    "    \n",
    "    def getAlpha(self):\n",
    "        return self.data[\"Alpha\"]\n",
    "    \n",
    "    def getDelta(self):\n",
    "        return self.data[\"Delta\"]\n",
    "    \n",
    "    def getTheta(self):\n",
    "        return self.data[\"Theta\"]\n",
    "    \n",
    "    def getLowBeta(self):\n",
    "        return self.data[\"Low Beta\"]\n",
    "    \n",
    "    def getHighBeta(self):\n",
    "        return self.data[\"High Beta\"]\n",
    "    \n",
    "    def getGamma(self):\n",
    "        return self.data[\"Gamma\"]\n",
    "\n",
    "    def get(self, key):\n",
    "        return self.data[key]\n",
    "\n",
    "    '''\n",
    "    Filter out all outliers, as defined by being outside 3*std from the mean, and replace with mean of the samples around them\n",
    "    '''\n",
    "    def filter_outliers(self):\n",
    "        sampleBad = False\n",
    "        for key in ['RawEEG', 'Alpha', 'Theta', 'Low Beta', 'High Beta', \"Gamma\", 'Delta']:\n",
    "            data = self.data[key]\n",
    "            \n",
    "            filtered = []\n",
    "\n",
    "            iqr = np.subtract(*np.percentile(data, [75, 25]))\n",
    "            med = np.median(data)\n",
    "\n",
    "            for x in data:\n",
    "                \n",
    "                if (med - 1.5*iqr > x) or (med + 1.5*iqr < x) or abs(x - np.mean(data)) > 2 * np.std(data):\n",
    "                    filtered.append(med)\n",
    "                    # filtered.append(np.median(data[max(0, i-5):i] + data[i+1:min(len(data), i+5)]))\n",
    "                else:\n",
    "                    filtered.append(x)\n",
    "                    \n",
    "            self.data[key] = filtered\n",
    "        return sampleBad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {personNum : {state: [sampleNums]}}\n",
    "# 0 = key for throwing away all samples of that state\n",
    "\n",
    "badSamples = {\n",
    "    1: {\"active\": [5], \"neutral\": [2], \"meditate\": []},\n",
    "    2: {\"active\": [0], \"neutral\": [0], \"meditate\": [0]},\n",
    "    3: {\"active\": [1, 4], \"neutral\": [1], \"meditate\": [5, 6, 7, 8]},\n",
    "    4: {\"active\": [2], \"neutral\": [1, 7], \"meditate\": [1, 8]},\n",
    "    5: {\"active\": [], \"neutral\": [], \"meditate\": []}, \n",
    "    6: {\"active\": [], \"neutral\": [2, 6], \"meditate\": []},\n",
    "    7: {\"active\": [5], \"neutral\": [4, 6, 7], \"meditate\": [1, 3, 4, 8]}, \n",
    "    8: {\"active\": [5], \"neutral\": [1], \"meditate\": [5, 8]}, \n",
    "    9: {\"active\": [], \"neutral\": [], \"meditate\": []}, \n",
    "    10: {\"active\": [6, 8], \"neutral\": [4, 5, 6], \"meditate\": []},\n",
    "    11: {\"active\": [4], \"neutral\": [4, 8], \"meditate\": [1, 2, 3, 5, 7]},\n",
    "    12: {\"active\": [2, 3, 8], \"neutral\": [0], \"meditate\": [6]}, \n",
    "    13: {\"active\": [], \"neutral\": [8], \"meditate\": []},\n",
    "    14: {\"active\": [4, 5, 8], \"neutral\": [0], \"meditate\": [1, 2, 8]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "dataLabels = []\n",
    "\n",
    "def transcribeFileToSample(personN: int, sampleN: int, state: str, X, y, outlierFiltering = True):\n",
    "    sample_data = Sample()\n",
    "\n",
    "    with open(\"data/all_data/\" + state + \"_\" + str(personN) + \"_\" + str(sampleN) + \".csv\") as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            sample_data.recordDataLine(row)\n",
    "\n",
    "        if (outlierFiltering):   \n",
    "            if (0 not in badSamples[personN][state] and sampleN not in badSamples[personN][state]):\n",
    "\n",
    "                for key in sample_data.data:\n",
    "                    sample_data.data[key] = sample_data.data[key][begin:end]\n",
    "\n",
    "                sample_data.filter_outliers()\n",
    "                X.append(sample_data)\n",
    "                y.append(state)\n",
    "        else:\n",
    "            X.append(sample_data)\n",
    "            y.append(state)\n",
    "\n",
    "for person in range(num_people):\n",
    "    for state in count_samples:\n",
    "        for i in range(count_samples[state]):\n",
    "            transcribeFileToSample(person + 1, i + 1, state, data, dataLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExtracted = []\n",
    "\n",
    "def safety_check(x):\n",
    "    if math.isnan(x): return 0\n",
    "    if math.isinf(x): return 99999999999\n",
    "    return x\n",
    "\n",
    "for point in data:\n",
    "    extractedPoint = []\n",
    "\n",
    "    extractedPoint.append(np.mean(point.getAlpha()))\n",
    "    extractedPoint.append(np.mean(point.getLowBeta()))\n",
    "    extractedPoint.append(np.mean(point.getHighBeta())) \n",
    "    extractedPoint.append(np.mean(point.getGamma())) \n",
    "    extractedPoint.append(np.mean(point.getTheta()))\n",
    "    extractedPoint.append(np.std(point.getHighBeta())) \n",
    "    extractedPoint.append(np.std(point.getGamma()))\n",
    "    extractedPoint.append(np.std(point.getDelta()))\n",
    "    extractedPoint.append(safety_check(ant.sample_entropy(point.getDelta())))\n",
    "    extractedPoint.append(np.std(point.getLowBeta())) \n",
    "    extractedPoint.append(np.std(point.getTheta()))\n",
    "    \n",
    "    # extractedPoint.append(safety_check(ant.spectral_entropy(point.getEEG(), sf=1)))\n",
    "    # extractedPoint.append(np.mean(point.getDelta()))\n",
    "    # extractedPoint.append(np.std(point.getAlpha())) \n",
    "\n",
    "    dataExtracted.append(extractedPoint)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataExtracted, dataLabels, test_size=0.2, random_state=13, stratify=dataLabels)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Training for Individual Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', LogisticRegression(max_iter=1000))]), \n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', KNeighborsClassifier())]),\n",
    "    Pipeline([('classifier', GaussianNB())]),\n",
    "    Pipeline([('classifier', BernoulliNB())]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', DecisionTreeClassifier())]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', RandomForestClassifier())]),\n",
    "    # Pipeline([('ss', StandardScaler()), ('classifier', XGBClassifier())]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', GradientBoostingClassifier())]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', SVC(probability=True, max_iter=1000))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, \n",
    "                                                max_features=None, min_samples_leaf=1, \n",
    "                                                min_samples_split=2, random_state=0)))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', SGDClassifier())]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', MLPClassifier())]),\n",
    "]\n",
    "\n",
    "model_grids = [\n",
    "               [{'classifier__C':[1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1, 5, 1e1, 5e1, 1e2, 5e2, 1e3],\n",
    "                 'classifier__random_state':[0]}],                                 #logistic regression\n",
    "               \n",
    "               [{'classifier__n_neighbors':[5,7,9,11, 13, 15, 17, 19], \n",
    "                 'classifier__metric': ['euclidean', 'manhattan', 'minkowski']}],  #KNN\n",
    "               \n",
    "               [{'classifier__var_smoothing': [1e-10, 1e-09, 1e-8, 1e-7]}],        #GaussianNB\n",
    "\n",
    "               [{'classifier__alpha': [1e-2, 1e-1, 1, 1e1, 1e2]}],                 #BernoulliNB\n",
    "\n",
    "               [{'classifier__criterion':['gini','entropy'],\n",
    "                 'classifier__random_state':[0], \n",
    "                 'classifier__max_depth' : [3, 5, 8, 10, 15, None], \n",
    "                 'classifier__min_samples_split' : [1.0,2,5,10,15,30],\n",
    "                 'classifier__min_samples_leaf': [1,2,5,10], \n",
    "                 'classifier__max_features': ['log2', 'sqrt', None]}],\n",
    "#Decision Tree\n",
    "               \n",
    "               [{'classifier__criterion':['gini','entropy'],\n",
    "                 'classifier__n_estimators': [1000],\n",
    "                 'classifier__random_state':[0], \n",
    "                 'classifier__max_depth' : [3, 5, 8, 10, 15, None], \n",
    "                 'classifier__min_samples_split' : [1.0,2,5,10,15,30],\n",
    "                 'classifier__min_samples_leaf': [1,2,5,10], \n",
    "                 'classifier__max_features': ['log2', 'sqrt', None]}],             #Random Forest\n",
    "\n",
    "              #  [{'classifier__n_estimators':[1000],\n",
    "              #    'classifier__criterion':['gini','entropy'],\n",
    "              #    'classifier__random_state':[0],\n",
    "              #    'classifier__max_depth': [3, 5, 8, 10, 15, 30],\n",
    "              #    'classifier__min_child_weight': [2,4,6,8,10],\n",
    "              #    'classifier__gamma': [0, 0.1, 0.2, 0.3],\n",
    "              #    'classifier__reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              #    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "              #    'classifier__eta': [0.1, 0.2, 0.3, 0.4, 0.5]}],                   #XGBoost\n",
    "\n",
    "                [{'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2], \n",
    "                 'classifier__n_estimators': [1000],\n",
    "                 'classifier__random_state':[0],\n",
    "                 'classifier__max_depth' : [5, 8, 15, None], \n",
    "                 'classifier__min_samples_split' : [1.0,2,5,10],\n",
    "                 'classifier__min_samples_leaf': [1,2,5,10], \n",
    "                 'classifier__max_features': ['log2', 'sqrt', 'auto', 'None']}],   #Gradient Boosting Decision Tree\n",
    "                        \n",
    "\n",
    "               [{'classifier__C':[1e-1, 1, 1e1] ,\n",
    "                 'classifier__random_state':[0],\n",
    "                 'classifier__kernel': ['rbf', 'poly']\n",
    "                }],                                                                #SVM\n",
    "               \n",
    "               [{'classifier__n_estimators' : [800, 1000, 1200], \n",
    "                 'classifier__learning_rate' : [1e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1, 1e1],\n",
    "                 'classifier__random_state':[0]}],                                  #AdaBoost\n",
    "\n",
    "                \n",
    "                [{\"classifier__max_iter\": [1000, 1500, 2000, 2500, 5000, 10000, 15000, 20000],\n",
    "                \"classifier__loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "                \"classifier__alpha\" : [0.01],\n",
    "                \"classifier__penalty\" : [\"l2\", \"l1\", \"none\"]}],                     #SGD\n",
    "\n",
    "                [{\"classifier__max_iter\": [1000, 1500, 2000, 2500, 5000, 10000],\n",
    "                \"classifier__solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "                \"classifier__alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "                \"classifier__hidden_layer_sizes\" : [(10,), (20,), (30,), (40,), (50,), (60,), (70,), (80,), (90,), (100,)]}], #MLP\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out so it doesn't run every time I run all and waste a lot of time\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings(\"ignore\")\n",
    "#     for i,j in zip(models, model_grids):\n",
    "#         grid = RandomizedSearchCV(estimator=i, param_distributions=j, n_iter = 100, scoring='f1_weighted', cv = skf, n_jobs=-1, verbose=2)\n",
    "#         grid.fit(X_train, y_train)\n",
    "#         best_f1 = grid.best_score_\n",
    "#         best_param = grid.best_params_\n",
    "#         print('{}:\\nBest F1 : {:.4f}'.format(i.steps[-1],best_f1))\n",
    "#         print('Best Parameters : ',best_param)\n",
    "#         print('')\n",
    "#         print('----------------')\n",
    "#         print('')\n",
    "\n",
    "# Logistic Regression: F1 0.567, C=500.0\n",
    "# KNN: F1 0.686, n_neighbors=5, metric=\"manhattan\"\n",
    "# GaussianNB: F1 0.540, var_smoothing = 1e-10\n",
    "# BernoulliNB: F1 0.194, alpha=0.01\n",
    "# Decision Tree: F1 0.694, min_samples_split=2, min_samples_leaf=2, max_features=log2, max_depth=None, criterion=entropy\n",
    "# Random Forest: F1 idk, n_estimators: 1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=None, criterion=entropy\n",
    "# Gradient Boosting: F1 0.767, n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=log2, max_depth=8, learning_rate=0.2\n",
    "# SVC: F1 0.67, kernel=rbf, C=10.0\n",
    "# Ada Boost: F1: 0.603, n_estimators=800, learning_rate=0.001\n",
    "# SGDClassifier: F1 0.5880, penalty=l2, max_iter=5000, loss=log, alpha=0.01\n",
    "# MLP: F1 0.7346, solver=adam, max_iter=2000, hidden_layer_sizes=(90,), alpha=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all models with their tuned hyperparameters\n",
    "models_tuned = [\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', LogisticRegression(max_iter=1000, C=500.0))]), \n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', KNeighborsClassifier(n_neighbors=5, metric=\"manhattan\"))]),\n",
    "    Pipeline([('classifier', GaussianNB(var_smoothing=1e-10))]),\n",
    "    Pipeline([('classifier', BernoulliNB(alpha=0.01))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=2, \n",
    "                                                                            max_features=\"log2\", criterion=\"entropy\"))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', RandomForestClassifier(n_estimators=1000, min_samples_split=2, \n",
    "                                                                            min_samples_leaf=1, max_features=\"sqrt\", criterion=\"entropy\"))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', GradientBoostingClassifier(n_estimators=1000, min_samples_split=2,\n",
    "                                                                                min_samples_leaf=2, max_features=\"log2\",\n",
    "                                                                                max_depth=8, learning_rate=0.2))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', SVC(probability=True, max_iter=1000, kernel=\"rbf\", C=10.0))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, \n",
    "                                                max_features=None, min_samples_leaf=1, \n",
    "                                                min_samples_split=2, random_state=0), n_estimators=800, learning_rate=0.001))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', SGDClassifier(penalty=\"l2\", max_iter=5000, loss=\"log\", alpha=0.01))]),\n",
    "    Pipeline([('ss', StandardScaler()), ('classifier', MLPClassifier(solver=\"adam\", max_iter=2000, hidden_layer_sizes=(90,), alpha=0.01))])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics/Scores for Individual Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model_stats = []\n",
    "\n",
    "for m in range(len(models_tuned)):\n",
    "    model_stats = []\n",
    "    model = models_tuned[m]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = skf)   #K-Fold Validation\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average= 'weighted')\n",
    "    recall = recall_score(y_test, y_pred, average= 'weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average= 'weighted')\n",
    "\n",
    "    predicted_probab = model.predict_proba(X_test)\n",
    "    predicted_probab = predicted_probab[:, 1]\n",
    "   \n",
    "    print(type(models_tuned[m][-1]).__name__ , ':')\n",
    "    \n",
    "    print('Accuracy Score: {:.4f}'.format(test_acc))\n",
    "    print(\"K-Fold Validation Mean Accuracy: {:.4f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Classification report: \")\n",
    "    print(cr)\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    print('')\n",
    "    model_stats.append(type(models_tuned[m][-1]).__name__)\n",
    "    model_stats.append(accuracies.mean())\n",
    "    model_stats.append(test_acc)\n",
    "    model_stats.append(precision)\n",
    "    model_stats.append(recall)\n",
    "    model_stats.append(f1)\n",
    "    tuned_model_stats.append(model_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tuned = pd.DataFrame(tuned_model_stats, columns= ['Model','Cross-val Acc','Test Accuracy','Precision','Recall', 'F1'])\n",
    "df_tuned.sort_values(by= ['F1'], inplace= True, ascending= False)\n",
    "df_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_classifiers = {\n",
    "    # ('log reg', Pipeline([('ss', StandardScaler()), ('classifier', LogisticRegression(max_iter=1000, C=500.0))])), \n",
    "    # ('gaussianNB', Pipeline([('classifier', GaussianNB(var_smoothing=1e-10))])),\n",
    "    # ('bernoulliNB', Pipeline([('classifier', BernoulliNB(alpha=0.01))])),\n",
    "    ('decision tree', Pipeline([('ss', StandardScaler()), ('classifier', DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=2, \n",
    "                                                                            max_features=\"log2\", criterion=\"entropy\"))])), \n",
    "    ('random forest', Pipeline([('ss', StandardScaler()), ('classifier', RandomForestClassifier(n_estimators=1000, min_samples_split=2, \n",
    "                                                                            min_samples_leaf=1, max_features=\"sqrt\", criterion=\"entropy\"))])), \n",
    "    ('gradient boosting', Pipeline([('ss', StandardScaler()), ('classifier', GradientBoostingClassifier(n_estimators=1000, min_samples_split=2,\n",
    "                                                                                min_samples_leaf=2, max_features=\"log2\",\n",
    "                                                                                max_depth=8, learning_rate=0.2))])), \n",
    "    # ('svc', Pipeline([('ss', StandardScaler()), ('classifier', SVC(probability=True, max_iter=1000, kernel=\"rbf\", C=10.0))])), \n",
    "    ('ada boost', Pipeline([('ss', StandardScaler()), ('classifier', AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, \n",
    "                                                max_features=None, min_samples_leaf=1, \n",
    "                                                min_samples_split=2, random_state=0), n_estimators=800, learning_rate=0.001))])),\n",
    "    ('mlp', Pipeline([('ss', StandardScaler()), ('classifier', MLPClassifier(solver=\"adam\", max_iter=2000, hidden_layer_sizes=(90,), alpha=0.01))])),\n",
    "\n",
    "}\n",
    "\n",
    "final_est = RandomForestClassifier(n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=\"sqrt\", criterion=\"entropy\", random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackingClf = StackingClassifier(estimators=indiv_classifiers, final_estimator=final_est)\n",
    "\n",
    "stackingClf.fit(X_train, y_train)\n",
    "y_pred = stackingClf.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "accuracies = cross_val_score(estimator = stackingClf, X = X_train, y = y_train, cv = skf)   #K-Fold Validation\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average= 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average= 'weighted')\n",
    "f1 = f1_score(y_test, y_pred, average= 'weighted')\n",
    "\n",
    "model_stats = []\n",
    "model_stats.append(type(stackingClf).__name__)\n",
    "model_stats.append(accuracies.mean())\n",
    "model_stats.append(test_acc)\n",
    "model_stats.append(precision)\n",
    "model_stats.append(recall)\n",
    "model_stats.append(f1)\n",
    "tuned_model_stats.append(model_stats)\n",
    "\n",
    "print('Accuracy Score: {:.4f}'.format(test_acc))\n",
    "print(\"K-Fold Validation Mean Accuracy: {:.4f} %\".format(accuracies.mean()*100))\n",
    "print(\"Classification report: \")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingClf = VotingClassifier(estimators=indiv_classifiers, voting='soft', weights = [1, 1.5, 1.4, 1, 1])\n",
    "# tree, forest, gradient, svc, ada\n",
    "\n",
    "votingClf.fit(X_train, y_train)\n",
    "y_pred = votingClf.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "accuracies = cross_val_score(estimator = votingClf, X = X_train, y = y_train, cv = skf, n_jobs=-1)   #K-Fold Validation\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average= 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average= 'weighted')\n",
    "f1 = f1_score(y_test, y_pred, average= 'weighted')\n",
    "\n",
    "model_stats = []\n",
    "model_stats.append(type(votingClf).__name__)\n",
    "model_stats.append(accuracies.mean())\n",
    "model_stats.append(test_acc)\n",
    "model_stats.append(precision)\n",
    "model_stats.append(recall)\n",
    "model_stats.append(f1)\n",
    "tuned_model_stats.append(model_stats)\n",
    "\n",
    "print('Accuracy Score: {:.4f}'.format(test_acc))\n",
    "print(\"K-Fold Validation Mean Accuracy: {:.4f}%\".format(accuracies.mean()*100))\n",
    "print(\"Classification report: \")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tuned = pd.DataFrame(tuned_model_stats, columns= ['Model','Cross-val Acc','Test Accuracy','Precision','Recall', 'F1'])\n",
    "df_tuned.sort_values(by= ['F1'], inplace= True, ascending= False)\n",
    "df_tuned"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
