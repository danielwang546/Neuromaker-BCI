{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import antropy as ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin, end = 1, 61 # (begin is inclusive, end is exclusive)\n",
    "num_people = 14\n",
    "count_samples = {\n",
    "    \"active\": 8,\n",
    "    \"meditate\": 8,\n",
    "    \"neutral\": 8\n",
    "}\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self):\n",
    "        self.data = {\n",
    "            'RawEEG': [],\n",
    "            'Alpha': [],\n",
    "            'Low Beta': [],\n",
    "            'High Beta': [],\n",
    "            'Gamma': [],\n",
    "            'Theta': [],\n",
    "            'Delta': [],\n",
    "            'Meditation': [],\n",
    "            'Attention': []\n",
    "        }\n",
    "\n",
    "    def recordDataPoint(self, RawEEG, Attention, Meditation, Alpha, Delta, Theta, LowBeta, HighBeta, Gamma):\n",
    "        self.data['RawEEG'].append(float(RawEEG))\n",
    "        self.data['Attention'].append(float(Attention))\n",
    "        self.data['Meditation'].append(float(Meditation))\n",
    "        self.data['Alpha'].append(float(Alpha))\n",
    "        self.data['Delta'].append(float(Delta))\n",
    "        self.data['Theta'].append(float(Theta))\n",
    "        self.data['Low Beta'].append(float(LowBeta))\n",
    "        self.data['High Beta'].append(float(HighBeta))\n",
    "        self.data['Gamma'].append(float(Gamma))\n",
    "\n",
    "    '''\n",
    "    Record a line of data from the CSV output, which takes form RawEEG, Alpha, Delta, Gamma, Low Beta, High Beta, Theta, Attention, Meditation\n",
    "\n",
    "    '''\n",
    "    def recordDataLine(self, line):\n",
    "        self.recordDataPoint(line[0], line[7], line[8], line[1], line[2], line[6], line[4], line[5], line[3])\n",
    "    \n",
    "    def getEEG(self):\n",
    "        return self.data['RawEEG']\n",
    "    \n",
    "    def getAttention(self):\n",
    "        return self.data[\"Attention\"]\n",
    "    \n",
    "    def getMeditation(self):\n",
    "        return self.data[\"Meditation\"]\n",
    "    \n",
    "    def getAlpha(self):\n",
    "        return self.data[\"Alpha\"]\n",
    "    \n",
    "    def getDelta(self):\n",
    "        return self.data[\"Delta\"]\n",
    "    \n",
    "    def getTheta(self):\n",
    "        return self.data[\"Theta\"]\n",
    "    \n",
    "    def getLowBeta(self):\n",
    "        return self.data[\"Low Beta\"]\n",
    "    \n",
    "    def getHighBeta(self):\n",
    "        return self.data[\"High Beta\"]\n",
    "    \n",
    "    def getGamma(self):\n",
    "        return self.data[\"Gamma\"]\n",
    "\n",
    "    def get(self, key):\n",
    "        return self.data[key]\n",
    "\n",
    "    '''\n",
    "    Filter out all outliers, as defined by being outside 3*std from the mean, and replace with mean of the samples around them\n",
    "    '''\n",
    "    def filter_outliers(self):\n",
    "        sampleBad = False\n",
    "        for key in ['RawEEG', 'Alpha', 'Theta', 'Low Beta', 'High Beta', \"Gamma\", 'Delta']:\n",
    "            data = self.data[key]\n",
    "            \n",
    "            filtered = []\n",
    "\n",
    "            iqr = np.subtract(*np.percentile(data, [75, 25]))\n",
    "            med = np.median(data)\n",
    "\n",
    "            for x in data:\n",
    "                \n",
    "                if (med - 1.5*iqr > x) or (med + 1.5*iqr < x) or abs(x - np.mean(data)) > 2 * np.std(data):\n",
    "                    filtered.append(med)\n",
    "                    # filtered.append(np.median(data[max(0, i-5):i] + data[i+1:min(len(data), i+5)]))\n",
    "                else:\n",
    "                    filtered.append(x)\n",
    "                    \n",
    "            self.data[key] = filtered\n",
    "        return sampleBad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {personNum : {state: [sampleNums]}}\n",
    "# 0 = key for throwing away all samples of that state\n",
    "\n",
    "badSamples = {\n",
    "    1: {\"active\": [5], \"neutral\": [2], \"meditate\": []},\n",
    "    2: {\"active\": [0], \"neutral\": [0], \"meditate\": [0]},\n",
    "    3: {\"active\": [1, 4], \"neutral\": [1], \"meditate\": [5, 6, 7, 8]},\n",
    "    4: {\"active\": [2], \"neutral\": [7], \"meditate\": [1, 8]}, # maybe n1\n",
    "    5: {\"active\": [], \"neutral\": [], \"meditate\": []}, # i love you person 5 \n",
    "    6: {\"active\": [], \"neutral\": [2, 6], \"meditate\": []},\n",
    "    7: {\"active\": [5], \"neutral\": [4, 6, 7], \"meditate\": [1, 3, 4, 8]}, # think about killing some of this data\n",
    "    8: {\"active\": [5], \"neutral\": [1], \"meditate\": []}, # maybe m5 and m8\n",
    "    9: {\"active\": [], \"neutral\": [], \"meditate\": []}, \n",
    "    10: {\"active\": [6, 8], \"neutral\": [4, 5, 6], \"meditate\": []},\n",
    "    11: {\"active\": [4], \"neutral\": [4, 8], \"meditate\": [1, 2, 3, 5, 7]},\n",
    "    12: {\"active\": [2, 3, 8], \"neutral\": [0], \"meditate\": [6]}, # maybe n0\n",
    "    13: {\"active\": [], \"neutral\": [8], \"meditate\": []},\n",
    "    14: {\"active\": [4, 5, 8], \"neutral\": [0], \"meditate\": [1, 2, 8]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "dataLabels = []\n",
    "\n",
    "def transcribeFileToSample(personN: int, sampleN: int, state: str, outlierFiltering: bool):\n",
    "    sample_data = Sample()\n",
    "\n",
    "    with open(\"data/all_data/\" + state + \"_\" + str(personN) + \"_\" + str(sampleN) + \".csv\") as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            sample_data.recordDataLine(row)\n",
    "\n",
    "        if (outlierFiltering):   \n",
    "            if (0 not in badSamples[personN][state] and sampleN not in badSamples[personN][state]):\n",
    "\n",
    "                for key in sample_data.data:\n",
    "                    sample_data.data[key] = sample_data.data[key][begin:end]\n",
    "\n",
    "                sample_data.filter_outliers()\n",
    "                data.append(sample_data)\n",
    "                dataLabels.append(state)\n",
    "        else:\n",
    "            data.append(sample_data)\n",
    "            dataLabels.append(state)\n",
    "\n",
    "for person in range(num_people - 1):\n",
    "    for state in count_samples:\n",
    "        for i in range(count_samples[state]):\n",
    "            transcribeFileToSample(person + 1, i + 1, state, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExtracted = []\n",
    "\n",
    "def safety_check(x):\n",
    "    if math.isnan(x): return 0\n",
    "    if math.isinf(x): return 99999999999\n",
    "    return x\n",
    "\n",
    "for point in data:\n",
    "    extractedPoint = []\n",
    "\n",
    "    extractedPoint.append(np.mean(point.getAlpha()))\n",
    "    extractedPoint.append(np.mean(point.getLowBeta()))\n",
    "    extractedPoint.append(np.mean(point.getHighBeta())) \n",
    "    extractedPoint.append(np.mean(point.getGamma())) \n",
    "    extractedPoint.append(np.mean(point.getTheta()))\n",
    "    extractedPoint.append(np.std(point.getHighBeta())) \n",
    "    extractedPoint.append(np.std(point.getGamma()))\n",
    "    extractedPoint.append(np.std(point.getDelta()))\n",
    "    extractedPoint.append(safety_check(ant.sample_entropy(point.getDelta())))\n",
    "    \n",
    "    # extractedPoint.append(safety_check(ant.spectral_entropy(point.getEEG(), sf=1)))\n",
    "    # extractedPoint.append(np.mean(point.getDelta()))\n",
    "    # extractedPoint.append(np.std(point.getAlpha())) \n",
    "    # extractedPoint.append(np.std(point.getLowBeta())) \n",
    "    # extractedPoint.append(np.std(point.getTheta()))\n",
    "\n",
    "    dataExtracted.append(extractedPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73 accuracy with a standard deviation of 0.10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cvclf = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=20, n_estimators=2000))\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "scores = cross_val_score(cvclf, dataExtracted, dataLabels, cv=cv, n_jobs=-1)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = 0\n",
    "# n = 10\n",
    "# errorLabels = {\"active\": {\"neutral\": 0, \"meditate\": 0}, \n",
    "#             \"neutral\": {\"active\": 0, \"meditate\": 0}, \n",
    "#             \"meditate\": {\"active\": 0, \"neutral\": 0}}\n",
    "#             # first key = actual, second key = prediction\n",
    "\n",
    "# for _ in range(n):\n",
    "#     train, test, trainLabels, testLabels = train_test_split(dataExtracted, dataLabels, test_size=0.20)\n",
    "#     clf = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=70, n_estimators=1800))\n",
    "#     clf.fit(train, trainLabels)\n",
    "\n",
    "#     predictions = clf.predict(test)\n",
    "#     for i in range(len(predictions)):\n",
    "#         if predictions[i] != testLabels[i]:\n",
    "#             errorLabels[testLabels[i]][predictions[i]] += 1 \n",
    "\n",
    "#     total += clf.score(test, testLabels) \n",
    "\n",
    "# print(total/n)\n",
    "# print(errorLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n",
      "261\n"
     ]
    }
   ],
   "source": [
    "print(len(dataExtracted))\n",
    "print(len(dataLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [237, 261]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mikad\\Documents\\GitHub\\Neuromaker-BCI\\Classifier_V2\\classifier_v2.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikad/Documents/GitHub/Neuromaker-BCI/Classifier_V2/classifier_v2.ipynb#ch0000006?line=45'>46</a>\u001b[0m     extractedPoint\u001b[39m.\u001b[39mappend(safety_check(ant\u001b[39m.\u001b[39msample_entropy(point\u001b[39m.\u001b[39mgetDelta())))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikad/Documents/GitHub/Neuromaker-BCI/Classifier_V2/classifier_v2.ipynb#ch0000006?line=47'>48</a>\u001b[0m     sampleX\u001b[39m.\u001b[39mappend(extractedPoint)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mikad/Documents/GitHub/Neuromaker-BCI/Classifier_V2/classifier_v2.ipynb#ch0000006?line=49'>50</a>\u001b[0m train, test, trainLabels, testLabels \u001b[39m=\u001b[39m train_test_split(dataExtracted, dataLabels, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.20\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikad/Documents/GitHub/Neuromaker-BCI/Classifier_V2/classifier_v2.ipynb#ch0000006?line=50'>51</a>\u001b[0m clf \u001b[39m=\u001b[39m make_pipeline(StandardScaler(), RandomForestClassifier(max_depth\u001b[39m=\u001b[39m\u001b[39m70\u001b[39m, n_estimators\u001b[39m=\u001b[39m\u001b[39m1800\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikad/Documents/GitHub/Neuromaker-BCI/Classifier_V2/classifier_v2.ipynb#ch0000006?line=51'>52</a>\u001b[0m clf\u001b[39m.\u001b[39mfit(train, trainLabels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/model_selection/_split.py?line=2413'>2414</a>\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/model_selection/_split.py?line=2414'>2415</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/model_selection/_split.py?line=2416'>2417</a>\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[0;32m   <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/model_selection/_split.py?line=2418'>2419</a>\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m   <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/model_selection/_split.py?line=2419'>2420</a>\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/model_selection/_split.py?line=2420'>2421</a>\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/model_selection/_split.py?line=2421'>2422</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=358'>359</a>\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=359'>360</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=360'>361</a>\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=373'>374</a>\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=374'>375</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=376'>377</a>\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=377'>378</a>\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=378'>379</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=329'>330</a>\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=330'>331</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=331'>332</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=332'>333</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=333'>334</a>\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    <a href='file:///c%3A/Users/mikad/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/sklearn/utils/validation.py?line=334'>335</a>\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [237, 261]"
     ]
    }
   ],
   "source": [
    "smallData = []\n",
    "smallDataLabels = []\n",
    "\n",
    "def transcribeFileToSampleSmall(state):\n",
    "    sample_data = Sample()\n",
    "    with open(\"data/all_data/\" + state + \"-small.csv\") as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            sample_data.recordDataLine(row)\n",
    "            \n",
    "        for key in sample_data.data:\n",
    "            sample_data.data[key] = sample_data.data[key][begin:end]\n",
    "\n",
    "        sample_data.filter_outliers()\n",
    "        smallData.append(sample_data)\n",
    "        smallDataLabels.append(state)\n",
    "\n",
    "# for state in ['active', 'meditate', 'neutral']:\n",
    "#     transcribeFileToSampleSmall(state)\n",
    "\n",
    "for state in count_samples:\n",
    "    for i in range(count_samples[state]):\n",
    "        transcribeFileToSample(14, i + 1, state, False)\n",
    "\n",
    "sampleX = []\n",
    "\n",
    "def safety_check(x):\n",
    "    if math.isnan(x): return 0\n",
    "    if math.isinf(x): return 99999999999\n",
    "    return x\n",
    "\n",
    "for point in smallData:\n",
    "    extractedPoint = []\n",
    "\n",
    "    extractedPoint.append(np.mean(point.getAlpha()))\n",
    "    extractedPoint.append(np.mean(point.getLowBeta()))\n",
    "    extractedPoint.append(np.mean(point.getHighBeta())) \n",
    "    extractedPoint.append(np.mean(point.getGamma())) \n",
    "    extractedPoint.append(np.mean(point.getTheta()))\n",
    "    extractedPoint.append(np.std(point.getHighBeta())) \n",
    "    extractedPoint.append(np.std(point.getGamma()))\n",
    "    extractedPoint.append(np.std(point.getDelta()))\n",
    "    extractedPoint.append(safety_check(ant.sample_entropy(point.getDelta())))\n",
    "\n",
    "    sampleX.append(extractedPoint)\n",
    "\n",
    "train, test, trainLabels, testLabels = train_test_split(dataExtracted, dataLabels, test_size=0.20)\n",
    "clf = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=70, n_estimators=1800))\n",
    "clf.fit(train, trainLabels)\n",
    "clf.predict(sampleX)\n",
    "clf.score(sampleX, smallDataLabels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cb6d1e253a7ab3e8a07d0a3da00ac1f2b77bfa839ee5db5b3eaf4eb76ff3570"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
